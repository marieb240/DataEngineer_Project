"""
Module principal du scraper VidIQ.
R√©cup√®re les donn√©es, les parse et les stocke dans MongoDB.
"""

import os
from pymongo import MongoClient
from datetime import datetime
from scrapers.http_client import HttpClient
from scrapers.vidiq_parser import VidIQParser


class VideoScraper:
    """
    Scraper complet pour VidIQ :
    1. R√©cup√®re la page top 100 via HTTP
    2. Parse le HTML avec BeautifulSoup
    3. Stocke dans MongoDB
    """
    
    def __init__(self):
        """Initialise le scraper avec les config."""
        # Config MongoDB depuis variables d'environnement
        self.mongo_host = os.getenv("MONGO_HOST", "localhost")
        self.mongo_port = int(os.getenv("MONGO_PORT", "27017"))
        self.mongo_db = os.getenv("MONGO_DB", "vidiq")
        self.mongo_user = os.getenv("MONGO_USER", "admin")
        self.mongo_pwd = os.getenv("MONGO_PASSWORD", "adminpass")
        
        # Client HTTP
        self.http_client = HttpClient()
        
        # URL √† scraper
        self.url = "https://vidiq.com/fr/youtube-stats/top/100/"
        
    def get_db(self):
        """Connecte √† MongoDB."""
        connection_string = (
            f"mongodb://{self.mongo_user}:{self.mongo_pwd}@"
            f"{self.mongo_host}:{self.mongo_port}/?authSource=admin"
        )
        client = MongoClient(connection_string)
        return client[self.mongo_db]
    
    def scrape_and_store(self):
        """
        Effectue le scraping complet :
        1. R√©cup√®re la page
        2. Parse les donn√©es
        3. Stocke dans Mongo
        
        Returns:
            bool: True si succ√®s, False sinon
        """
        try:
            print("\n" + "="*60)
            print("üöÄ D√©marrage du scraping VidIQ Top 100")
            print("="*60)
            
            # Step 1 : R√©cup√®re la page
            print(f"\nüì• √âtape 1 : R√©cup√©ration de {self.url}")
            response = self.http_client.get(self.url)
            
            # Step 2 : Parse le HTML
            print("\nüìä √âtape 2 : Parsing du HTML")
            channels = VidIQParser.parse_top_100(response.text)
            
            if not channels:
                print("‚úó Aucune donn√©e extraite")
                return False
            
            # Step 3 : Ajoute timestamp et stocke dans Mongo
            print("\nüíæ √âtape 3 : Stockage dans MongoDB")
            db = self.get_db()
            collection = db['channels']
            
            # Marque chaque document avec la date de scraping
            for channel in channels:
                channel['scraped_at'] = datetime.utcnow()
                channel['_id'] = f"{channel['rank']}_{datetime.utcnow().strftime('%Y%m%d')}"
            
            # Ins√®re les donn√©es (remplace si d√©j√† existantes)
            result = collection.insert_many(channels, ordered=False)
            
            print(f"[VideoScraper] ‚úì {len(result.inserted_ids)} documents ins√©r√©s")
            
            # Affiche un r√©sum√©
            print("\nüìà R√©sum√© des top 5 :")
            for channel in channels[:5]:
                print(f"  #{channel['rank']} - {channel['name']}")
                print(f"     Abonn√©s: {channel['subscribers']:,}")
                print(f"     Vues: {channel['total_views']:,}")
            
            print("\n" + "="*60)
            print("‚úÖ Scraping termin√© avec succ√®s !")
            print("="*60 + "\n")
            
            return True
            
        except Exception as e:
            print(f"\n‚úó Erreur lors du scraping : {e}")
            import traceback
            traceback.print_exc()
            return False


def main():
    """Point d'entr√©e pour lancer le scraper."""
    scraper = VideoScraper()
    success = scraper.scrape_and_store()
    return 0 if success else 1


if __name__ == "__main__":
    exit(main())
